{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "import os\n",
    "from openai import OpenAIError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.max_budget = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: 이거 프로덕션에서 노출 안되도록 수정해야함\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging\n",
    "litellm.set_verbose = True\n",
    "litellm.json_logs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: pofgiru.\n",
      "View Weave data at https://wandb.ai/pofgiru/LiteLLM/weave\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"de4cf35cae9134f7844536f2849ecd4bae1d545b\"\n",
    "weave.init('LiteLLM')\n",
    "litellm.success_callback = [\"weave\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='gpt-3.5-turbo', messages=[{'content': '안녕', 'role': 'user'}])\u001b[0m\n",
      "\n",
      "\n",
      "init callback list: weave\n",
      "self.optional_params: {}\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'extra_body': {}}\n",
      "self.optional_params: {'extra_body': {}}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-d '{'model': 'gpt-3.5-turbo', 'messages': [{'content': '안녕', 'role': 'user'}], 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-9WeiCTi4DmKRP02UWUNdeMhqMTNCD\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"\\uc548\\ub155\\ud558\\uc138\\uc694! \\ubb34\\uc5c7\\uc744 \\ub3c4\\uc640\\ub4dc\\ub9b4\\uae4c\\uc694?\", \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}}], \"created\": 1717570020, \"model\": \"gpt-3.5-turbo-0125\", \"object\": \"chat.completion\", \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 21, \"prompt_tokens\": 11, \"total_tokens\": 32}}\n",
      "\n",
      "\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "Looking up model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "Success: model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "prompt_tokens=11; completion_tokens=21\n",
      "Returned custom cost for model=gpt-3.5-turbo-0125 - prompt_tokens_cost_usd_dollar: 5.5e-06, completion_tokens_cost_usd_dollar: 3.15e-05\n",
      "final cost: 3.7e-05; prompt_tokens_cost_usd_dollar: 5.5e-06; completion_tokens_cost_usd_dollar: 3.15e-05\n",
      "success callbacks: ['weave']\n",
      "response: \n",
      "안녕하세요! 무엇을 도와드릴까요?\n",
      "\n",
      "\n",
      "chunks: \n",
      "('id', 'chatcmpl-9WeiCTi4DmKRP02UWUNdeMhqMTNCD')\n",
      "('choices', [Choices(finish_reason='stop', index=0, message=Message(content='안녕하세요! 무엇을 도와드릴까요?', role='assistant'))])\n",
      "('created', 1717570020)\n",
      "('model', 'gpt-3.5-turbo-0125')\n",
      "('object', 'chat.completion')\n",
      "('system_fingerprint', None)\n",
      "('usage', Usage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "raw response: \n",
      "ModelResponse(id='chatcmpl-9WeiCTi4DmKRP02UWUNdeMhqMTNCD', choices=[Choices(finish_reason='stop', index=0, message=Message(content='안녕하세요! 무엇을 도와드릴까요?', role='assistant'))], created=1717570020, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  response = completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{ \"content\": \"안녕\",\"role\": \"user\"}],\n",
    "    # stream=True\n",
    "  )\n",
    "except OpenAIError as e:\n",
    "  print('error')\n",
    "  print(e)\n",
    "  print('/error')\n",
    "\n",
    "# print(\"\\n\")\n",
    "print(\"response: \")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"chunks: \")\n",
    "for chunk in response:\n",
    "  print(chunk)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"raw response: \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='gpt-3.5-turbo', messages=[{'content': '안녕', 'role': 'user'}], temperature=0)\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0, 'extra_body': {}}\n",
      "self.optional_params: {'temperature': 0, 'extra_body': {}}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-d '{'model': 'gpt-3.5-turbo', 'messages': [{'content': '안녕', 'role': 'user'}], 'temperature': 0, 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-9WeiEA1uGLOobtttujP05xGivpcYm\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"\\uc548\\ub155\\ud558\\uc138\\uc694! \\ubb34\\uc5c7\\uc744 \\ub3c4\\uc640\\ub4dc\\ub9b4\\uae4c\\uc694?\", \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}}], \"created\": 1717570022, \"model\": \"gpt-3.5-turbo-0125\", \"object\": \"chat.completion\", \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 21, \"prompt_tokens\": 11, \"total_tokens\": 32}}\n",
      "\n",
      "\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "Looking up model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "Success: model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "prompt_tokens=11; completion_tokens=21\n",
      "Returned custom cost for model=gpt-3.5-turbo-0125 - prompt_tokens_cost_usd_dollar: 5.5e-06, completion_tokens_cost_usd_dollar: 3.15e-05\n",
      "final cost: 3.7e-05; prompt_tokens_cost_usd_dollar: 5.5e-06; completion_tokens_cost_usd_dollar: 3.15e-05\n",
      "success callbacks: ['weave']\n",
      "ModelResponse(id='chatcmpl-9WeiEA1uGLOobtttujP05xGivpcYm', choices=[Choices(finish_reason='stop', index=0, message=Message(content='안녕하세요! 무엇을 도와드릴까요?', role='assistant'))], created=1717570022, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n",
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='gpt-3.5-turbo', messages=[{'content': '안녕 스폰지밥? (대답은 스폰지밥처럼 하시오)', 'role': 'user'}], temperature=0)\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0, 'extra_body': {}}\n",
      "self.optional_params: {'temperature': 0, 'extra_body': {}}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-d '{'model': 'gpt-3.5-turbo', 'messages': [{'content': '안녕 스폰지밥? (대답은 스폰지밥처럼 하시오)', 'role': 'user'}], 'temperature': 0, 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-9WeiFzZD4a6fu5fECuxiwRP5dbg6I\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"\\uc548\\ub155 \\ub098 \\ud574\\uace0!\", \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}}], \"created\": 1717570023, \"model\": \"gpt-3.5-turbo-0125\", \"object\": \"chat.completion\", \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 8, \"prompt_tokens\": 39, \"total_tokens\": 47}}\n",
      "\n",
      "\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "Looking up model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "Success: model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "prompt_tokens=39; completion_tokens=8\n",
      "Returned custom cost for model=gpt-3.5-turbo-0125 - prompt_tokens_cost_usd_dollar: 1.95e-05, completion_tokens_cost_usd_dollar: 1.2e-05\n",
      "final cost: 3.15e-05; prompt_tokens_cost_usd_dollar: 1.95e-05; completion_tokens_cost_usd_dollar: 1.2e-05\n",
      "success callbacks: ['weave']\n",
      "안녕 나 해고!\n"
     ]
    }
   ],
   "source": [
    "def getResponse (message, *args, **kwargs):\n",
    "\t return completion(model=\"gpt-3.5-turbo\", messages=[{ \"content\": message, \"role\": \"user\"}], temperature=0, *args, **kwargs)\n",
    "\n",
    "print(getResponse(\"안녕\"))\n",
    "\n",
    "def getOnlyTextResponse (response):\n",
    "\t return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(getOnlyTextResponse(getResponse(\"안녕 스폰지밥? (대답은 스폰지밥처럼 하시오)\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='gpt-3.5-turbo', messages=[{'content': '안 스지? (대겨 얼굴 먹기 위해 잠시 걸어봐 주세요)', 'role': 'user'}], temperature=0)\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0, 'extra_body': {}}\n",
      "self.optional_params: {'temperature': 0, 'extra_body': {}}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-d '{'model': 'gpt-3.5-turbo', 'messages': [{'content': '안 스지? (대겨 얼굴 먹기 위해 잠시 걸어봐 주세요)', 'role': 'user'}], 'temperature': 0, 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-9WeiG40yDq6r5m5SI5bjZj628MKyQ\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"\\uc8c4\\uc1a1\\ud569\\ub2c8\\ub2e4, \\uc81c\\uac00 \\uc774\\ud574\\ud558\\uc9c0 \\ubabb\\ud588\\ub098\\ubd10\\uc694. \\uc880 \\ub354 \\uc790\\uc138\\ud788 \\uc124\\uba85\\ud574\\uc8fc\\uc2dc\\uaca0\\uc5b4\\uc694?\", \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}}], \"created\": 1717570024, \"model\": \"gpt-3.5-turbo-0125\", \"object\": \"chat.completion\", \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 43, \"prompt_tokens\": 39, \"total_tokens\": 82}}\n",
      "\n",
      "\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "Looking up model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "Success: model=gpt-3.5-turbo-0125 in model_cost_map\n",
      "prompt_tokens=39; completion_tokens=43\n",
      "Returned custom cost for model=gpt-3.5-turbo-0125 - prompt_tokens_cost_usd_dollar: 1.95e-05, completion_tokens_cost_usd_dollar: 6.45e-05\n",
      "final cost: 8.4e-05; prompt_tokens_cost_usd_dollar: 1.95e-05; completion_tokens_cost_usd_dollar: 6.45e-05\n",
      "success callbacks: ['weave']\n",
      "죄송합니다, 제가 이해하지 못했나봐요. 좀 더 자세히 설명해주시겠어요?\n"
     ]
    }
   ],
   "source": [
    "print(getOnlyTextResponse(getResponse(\"안 스지? (대겨 얼굴 먹기 위해 잠시 걸어봐 주세요)\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='gpt-3.5-turbo', messages=[{'content': '안녕 스폰지밥? (대답은 징징이처럼 하시오)', 'role': 'user'}], temperature=0, stream=True)\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'temperature': 0, 'stream': True, 'extra_body': {}}\n",
      "self.optional_params: {'temperature': 0, 'stream': True, 'extra_body': {}}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-H 'Authorization: Bearer sk-C7BlUyLeMZdyTbBkRtkpT3BlbkFJ********************' \\\n",
      "-d '{'model': 'gpt-3.5-turbo', 'messages': [{'content': '안녕 스폰지밥? (대답은 징징이처럼 하시오)', 'role': 'user'}], 'temperature': 0, 'stream': True, 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "RAW RESPONSE:\n",
      "<litellm.utils.CustomStreamWrapper object at 0x000001C710718D10>\n",
      "\n",
      "\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: \n",
      "model_response finish reason 3: None; response_obj={'text': '', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '', 'function_call': None, 'role': 'assistant', 'tool_calls': None}\n",
      "new delta: Delta(content='', role='assistant', function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='', role='assistant', function_call=None, tool_calls=None); completion_obj: {'content': ''}\n",
      "self.sent_first_chunk: False\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: None\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='징', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='징', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: 징\n",
      "model_response finish reason 3: None; response_obj={'text': '징', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='징', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '징', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content='징', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='징', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '징'}\n",
      "self.sent_first_chunk: False\n",
      "hold - False, model_response_str - 징\n",
      "choice_json: {'delta': {'content': '징', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='징', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: False\n",
      "model_response.choices[0].delta: Delta(content='징', role='assistant', function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='징', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1717570026, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='징', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1717570026, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "징\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='징', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='징', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: 징\n",
      "model_response finish reason 3: None; response_obj={'text': '징', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='징', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '징', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content='징', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='징', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '징'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - 징\n",
      "choice_json: {'delta': {'content': '징', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='징', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content='징', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='징', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570026, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='징', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570026, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "징\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: !\n",
      "model_response finish reason 3: None; response_obj={'text': '!', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content='!', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '!'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - !\n",
      "choice_json: {'delta': {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570026, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570026, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "!\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: \n",
      "model_response finish reason 3: stop; response_obj={'text': '', 'is_finished': True, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1717570026, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': None, 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=None, role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=None, role=None, function_call=None, tool_calls=None); completion_obj: {'content': ''}\n",
      "self.sent_first_chunk: True\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiIqq0WSLyZugxsYwASRIOMrRCt', choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(content=None, role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570026, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'content': '안녕 스폰지밥? (대답은 징징이처럼 하시오)', 'role': 'user'}]\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n"
     ]
    }
   ],
   "source": [
    "def printOnlyTextResponseStreaming(message, *args, **kwargs):\n",
    "\tresponse = getResponse(message, stream=True, *args, **kwargs)\n",
    "\tfor part in response:\n",
    "\t\tprint(part.choices[0].delta.content or \"\")\n",
    "\n",
    "printOnlyTextResponseStreaming(\"안녕 스폰지밥? (대답은 징징이처럼 하시오)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92mlitellm.completion(model='gpt-3.5-turbo', messages=[{'role': 'user', 'content': \"Hey, how's it going?\"}], stream=True)\u001b[0m\n",
      "\n",
      "\n",
      "self.optional_params: {}\n",
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {'stream': True, 'extra_body': {}}\n",
      "self.optional_params: {'stream': True, 'extra_body': {}}\n",
      "\u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.openai.com/v1/ \\\n",
      "-H 'Authorization: Bearer sk-C7BlUyLeMZdyTbBkRtkpT3BlbkFJ********************' \\\n",
      "-d '{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': \"Hey, how's it going?\"}], 'stream': True, 'extra_body': {}}'\n",
      "\u001b[0m\n",
      "\n",
      "RAW RESPONSE:\n",
      "<litellm.utils.CustomStreamWrapper object at 0x000001C70A86BB60>\n",
      "\n",
      "\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: \n",
      "model_response finish reason 3: None; response_obj={'text': '', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '', 'function_call': None, 'role': 'assistant', 'tool_calls': None}\n",
      "new delta: Delta(content='', role='assistant', function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='', role='assistant', function_call=None, tool_calls=None); completion_obj: {'content': ''}\n",
      "self.sent_first_chunk: False\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: None\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: Hello\n",
      "model_response finish reason 3: None; response_obj={'text': 'Hello', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='Hello', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': 'Hello', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content='Hello', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='Hello', role=None, function_call=None, tool_calls=None); completion_obj: {'content': 'Hello'}\n",
      "self.sent_first_chunk: False\n",
      "hold - False, model_response_str - Hello\n",
      "choice_json: {'delta': {'content': 'Hello', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Hello', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: False\n",
      "model_response.choices[0].delta: Delta(content='Hello', role='assistant', function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Hello', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='Hello', role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: !\n",
      "model_response finish reason 3: None; response_obj={'text': '!', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content='!', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '!'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - !\n",
      "choice_json: {'delta': {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  I\n",
      "model_response finish reason 3: None; response_obj={'text': ' I', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' I'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  I\n",
      "choice_json: {'delta': {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=\"'m\", function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=\"'m\", function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: 'm\n",
      "model_response finish reason 3: None; response_obj={'text': \"'m\", 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=\"'m\", function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': \"'m\", 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=\"'m\", role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=\"'m\", role=None, function_call=None, tool_calls=None); completion_obj: {'content': \"'m\"}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - 'm\n",
      "choice_json: {'delta': {'content': \"'m\", 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=\"'m\", role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=\"'m\", role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=\"'m\", role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=\"'m\", role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' just', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' just', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  just\n",
      "model_response finish reason 3: None; response_obj={'text': ' just', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' just', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' just', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' just', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' just', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' just'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  just\n",
      "choice_json: {'delta': {'content': ' just', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' just', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' just', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' just', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' just', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  a\n",
      "model_response finish reason 3: None; response_obj={'text': ' a', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' a', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' a'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  a\n",
      "choice_json: {'delta': {'content': ' a', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' a', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' a', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' virtual', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' virtual', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  virtual\n",
      "model_response finish reason 3: None; response_obj={'text': ' virtual', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' virtual', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' virtual', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' virtual', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' virtual', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' virtual'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  virtual\n",
      "choice_json: {'delta': {'content': ' virtual', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' virtual', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' virtual', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' virtual', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' virtual', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' assistant', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' assistant', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  assistant\n",
      "model_response finish reason 3: None; response_obj={'text': ' assistant', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' assistant', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' assistant', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' assistant', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' assistant', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' assistant'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  assistant\n",
      "choice_json: {'delta': {'content': ' assistant', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assistant', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' assistant', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assistant', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assistant', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: ,\n",
      "model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - ,\n",
      "choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' so', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' so', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  so\n",
      "model_response finish reason 3: None; response_obj={'text': ' so', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' so', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' so', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' so', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' so', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' so'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  so\n",
      "choice_json: {'delta': {'content': ' so', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' so', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' so', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' so', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' so', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  I\n",
      "model_response finish reason 3: None; response_obj={'text': ' I', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' I'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  I\n",
      "choice_json: {'delta': {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' don', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' don', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  don\n",
      "model_response finish reason 3: None; response_obj={'text': ' don', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' don', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' don', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' don', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' don', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' don'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  don\n",
      "choice_json: {'delta': {'content': ' don', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' don', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' don', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' don', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' don', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=\"'t\", function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=\"'t\", function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: 't\n",
      "model_response finish reason 3: None; response_obj={'text': \"'t\", 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=\"'t\", function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': \"'t\", 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=\"'t\", role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=\"'t\", role=None, function_call=None, tool_calls=None); completion_obj: {'content': \"'t\"}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - 't\n",
      "choice_json: {'delta': {'content': \"'t\", 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=\"'t\", role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=\"'t\", role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=\"'t\", role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=\"'t\", role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' have', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' have', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  have\n",
      "model_response finish reason 3: None; response_obj={'text': ' have', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' have', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' have', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' have', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' have', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' have'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  have\n",
      "choice_json: {'delta': {'content': ' have', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' have', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' have', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' have', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' have', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' feelings', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' feelings', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  feelings\n",
      "model_response finish reason 3: None; response_obj={'text': ' feelings', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' feelings', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' feelings', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' feelings', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' feelings', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' feelings'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  feelings\n",
      "choice_json: {'delta': {'content': ' feelings', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' feelings', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' feelings', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' feelings', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' feelings', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: ,\n",
      "model_response finish reason 3: None; response_obj={'text': ',', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=',', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=',', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ','}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - ,\n",
      "choice_json: {'delta': {'content': ',', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=',', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=',', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' but', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' but', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  but\n",
      "model_response finish reason 3: None; response_obj={'text': ' but', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' but', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' but', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' but', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' but', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' but'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  but\n",
      "choice_json: {'delta': {'content': ' but', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' but', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' but', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' but', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' but', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' thank', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' thank', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  thank\n",
      "model_response finish reason 3: None; response_obj={'text': ' thank', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' thank', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' thank', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' thank', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' thank', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' thank'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  thank\n",
      "choice_json: {'delta': {'content': ' thank', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' thank', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' thank', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' thank', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' thank', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  you\n",
      "model_response finish reason 3: None; response_obj={'text': ' you', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' you', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' you', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' you', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' you'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  you\n",
      "choice_json: {'delta': {'content': ' you', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' you', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  for\n",
      "model_response finish reason 3: None; response_obj={'text': ' for', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' for', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' for'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  for\n",
      "choice_json: {'delta': {'content': ' for', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' for', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' for', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' asking', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' asking', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  asking\n",
      "model_response finish reason 3: None; response_obj={'text': ' asking', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' asking', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' asking', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' asking', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' asking', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' asking'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  asking\n",
      "choice_json: {'delta': {'content': ' asking', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' asking', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' asking', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' asking', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' asking', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: !\n",
      "model_response finish reason 3: None; response_obj={'text': '!', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='!', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content='!', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '!'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - !\n",
      "choice_json: {'delta': {'content': '!', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content='!', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='!', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  How\n",
      "model_response finish reason 3: None; response_obj={'text': ' How', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' How', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' How', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' How', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' How', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' How'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  How\n",
      "choice_json: {'delta': {'content': ' How', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' How', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' How', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' How', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' How', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  can\n",
      "model_response finish reason 3: None; response_obj={'text': ' can', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' can', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' can', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' can', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' can', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' can'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  can\n",
      "choice_json: {'delta': {'content': ' can', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' can', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' can', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' can', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' can', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  I\n",
      "model_response finish reason 3: None; response_obj={'text': ' I', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' I', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' I'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  I\n",
      "choice_json: {'delta': {'content': ' I', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' I', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' I', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  assist\n",
      "model_response finish reason 3: None; response_obj={'text': ' assist', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' assist', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' assist', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' assist', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' assist', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' assist'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  assist\n",
      "choice_json: {'delta': {'content': ' assist', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assist', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' assist', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assist', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' assist', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  you\n",
      "model_response finish reason 3: None; response_obj={'text': ' you', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' you', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' you', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' you', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' you', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' you'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  you\n",
      "choice_json: {'delta': {'content': ' you', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' you', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' you', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content:  today\n",
      "model_response finish reason 3: None; response_obj={'text': ' today', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=' today', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': ' today', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=' today', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=' today', role=None, function_call=None, tool_calls=None); completion_obj: {'content': ' today'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str -  today\n",
      "choice_json: {'delta': {'content': ' today', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' today', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content=' today', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' today', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content=' today', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: ?\n",
      "model_response finish reason 3: None; response_obj={'text': '?', 'is_finished': False, 'finish_reason': None, 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content='?', function_call=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': '?', 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content='?', role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content='?', role=None, function_call=None, tool_calls=None); completion_obj: {'content': '?'}\n",
      "self.sent_first_chunk: True\n",
      "hold - False, model_response_str - ?\n",
      "choice_json: {'delta': {'content': '?', 'function_call': None, 'role': None, 'tool_calls': None}, 'index': 0, 'logprobs': None}\n",
      "choices in streaming: [StreamingChoices(finish_reason=None, index=0, delta=Delta(content='?', role=None, function_call=None, tool_calls=None), logprobs=None)]\n",
      "self.sent_first_chunk: True\n",
      "model_response.choices[0].delta: Delta(content='?', role=None, function_call=None, tool_calls=None)\n",
      "returning model_response: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='?', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(content='?', role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "PROCESSED CHUNK PRE CHUNK CREATOR: ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None); custom_llm_provider: openai\n",
      "\n",
      "Raw OpenAI Chunk\n",
      "ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "\n",
      "completion obj content: \n",
      "model_response finish reason 3: stop; response_obj={'text': '', 'is_finished': True, 'finish_reason': 'stop', 'logprobs': None, 'original_chunk': ChatCompletionChunk(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1717570027, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None), 'usage': None}\n",
      "original delta: {'content': None, 'function_call': None, 'role': None, 'tool_calls': None}\n",
      "new delta: Delta(content=None, role=None, function_call=None, tool_calls=None)\n",
      "model_response.choices[0].delta: Delta(content=None, role=None, function_call=None, tool_calls=None); completion_obj: {'content': ''}\n",
      "self.sent_first_chunk: True\n",
      "PROCESSED CHUNK POST CHUNK CREATOR: ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(content=None, role=None, function_call=None, tool_calls=None), logprobs=None)], created=1717570027, model='gpt-3.5-turbo', object='chat.completion.chunk', system_fingerprint=None)\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'role': 'user', 'content': \"Hey, how's it going?\"}]\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'role': 'user', 'content': \"Hey, how's it going?\"}]\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "ModelResponse(id='chatcmpl-9WeiJvy6GM4JMH3aroQLSCBL8Hf5i', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! I'm just a virtual assistant, so I don't have feelings, but thank you for asking! How can I assist you today?\", role='assistant'))], created=1717570027, model='gpt-3.5-turbo', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=29, prompt_tokens=14, total_tokens=43))\n",
      "Hello! I'm just a virtual assistant, so I don't have feelings, but thank you for asking! How can I assist you today?\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Async success callbacks: Got a complete streaming response\n",
      "Looking up model=gpt-3.5-turbo in model_cost_map\n",
      "Success: model=gpt-3.5-turbo in model_cost_map\n",
      "prompt_tokens=14; completion_tokens=28\n",
      "Returned custom cost for model=gpt-3.5-turbo - prompt_tokens_cost_usd_dollar: 2.1000000000000002e-05, completion_tokens_cost_usd_dollar: 5.6e-05\n",
      "final cost: 7.7e-05; prompt_tokens_cost_usd_dollar: 2.1000000000000002e-05; completion_tokens_cost_usd_dollar: 5.6e-05\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Logging Details LiteLLM-Async Success Call\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'role': 'user', 'content': \"Hey, how's it going?\"}]\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Async success callbacks: Got a complete streaming response\n",
      "Looking up model=gpt-3.5-turbo in model_cost_map\n",
      "Success: model=gpt-3.5-turbo in model_cost_map\n",
      "prompt_tokens=37; completion_tokens=5\n",
      "Returned custom cost for model=gpt-3.5-turbo - prompt_tokens_cost_usd_dollar: 5.55e-05, completion_tokens_cost_usd_dollar: 9.999999999999999e-06\n",
      "final cost: 6.549999999999999e-05; prompt_tokens_cost_usd_dollar: 5.55e-05; completion_tokens_cost_usd_dollar: 9.999999999999999e-06\n",
      "Logging Details LiteLLM-Success Call: None\n",
      "success callbacks: ['weave']\n",
      "Goes into checking if chunk has hiddden created at param\n",
      "Chunks have a created at hidden param\n",
      "Chunks sorted\n",
      "token_counter messages received: [{'content': '안녕 스폰지밥? (대답은 징징이처럼 하시오)', 'role': 'user'}]\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Token Counter - using OpenAI token counter, for model=gpt-3.5-turbo\n",
      "LiteLLM: Utils - Counting tokens for OpenAI model=gpt-3.5-turbo\n",
      "Logging Details LiteLLM-Success Call streaming complete\n",
      "Looking up model=gpt-3.5-turbo in model_cost_map\n",
      "Success: model=gpt-3.5-turbo in model_cost_map\n",
      "prompt_tokens=37; completion_tokens=5\n",
      "Returned custom cost for model=gpt-3.5-turbo - prompt_tokens_cost_usd_dollar: 5.55e-05, completion_tokens_cost_usd_dollar: 9.999999999999999e-06\n",
      "final cost: 6.549999999999999e-05; prompt_tokens_cost_usd_dollar: 5.55e-05; completion_tokens_cost_usd_dollar: 9.999999999999999e-06\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hey, how's it going?\"}]\n",
    "response = completion(model=\"gpt-3.5-turbo\", messages=messages, stream=True)\n",
    "\n",
    "chunks = []\n",
    "for chunk in response:\n",
    "    chunks.append(chunk)\n",
    "\n",
    "newResponse = litellm.stream_chunk_builder(chunks, messages=messages)\n",
    "\n",
    "print(newResponse)\n",
    "\n",
    "print(getOnlyTextResponse(newResponse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Details LiteLLM-Success Call streaming complete\n",
      "Looking up model=gpt-3.5-turbo in model_cost_map\n",
      "Success: model=gpt-3.5-turbo in model_cost_map\n",
      "prompt_tokens=14; completion_tokens=28\n",
      "Returned custom cost for model=gpt-3.5-turbo - prompt_tokens_cost_usd_dollar: 2.1000000000000002e-05, completion_tokens_cost_usd_dollar: 5.6e-05\n",
      "final cost: 7.7e-05; prompt_tokens_cost_usd_dollar: 2.1000000000000002e-05; completion_tokens_cost_usd_dollar: 5.6e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ss'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "'ss'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
