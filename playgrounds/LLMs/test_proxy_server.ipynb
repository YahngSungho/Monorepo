{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "import os\n",
    "from openai import OpenAIError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.max_budget = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set ENV variables\n",
    "# Fix: 이거 프록시 서버로 하면 생략 가능?\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track_cost_callback\n",
    "def track_cost_callback(\n",
    "    kwargs,                 # kwargs to completion\n",
    "    completion_response,    # response from completion\n",
    "    start_time, end_time    # start/end time\n",
    "):\n",
    "    try:\n",
    "      response_cost = kwargs.get(\"response_cost\", 0)\n",
    "      print(\"\\n\")\n",
    "      print(\"streaming start_time\", start_time)\n",
    "      print(\"streaming end_time\", end_time)\n",
    "      print(\"streaming response_cost\", response_cost)\n",
    "      print(\"\\n\")\n",
    "    except OpenAIError as e:\n",
    "      print(\"Error\", e)\n",
    "# set callback\n",
    "litellm.success_callback = [track_cost_callback] # set custom callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.set_verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.set_verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "response: \n",
      "안녕하세요! 무엇을 도와드릴까요?\n",
      "\n",
      "\n",
      "chunks: \n",
      "('id', 'chatcmpl-9UCShNNhyEPCVjgwkYSeqvkZJPNbR')\n",
      "('choices', [Choices(finish_reason='stop', index=0, message=Message(content='안녕하세요! 무엇을 도와드릴까요?', role='assistant'))])\n",
      "('created', 1716984771)\n",
      "('model', 'gpt-3.5-turbo-0125')\n",
      "('object', 'chat.completion')\n",
      "('system_fingerprint', None)\n",
      "('usage', Usage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "raw response: \n",
      "ModelResponse(id='chatcmpl-9UCShNNhyEPCVjgwkYSeqvkZJPNbR', choices=[Choices(finish_reason='stop', index=0, message=Message(content='안녕하세요! 무엇을 도와드릴까요?', role='assistant'))], created=1716984771, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n",
      "\n",
      "streaming start_time 2024-05-29 21:12:50.111037\n",
      "streaming end_time 2024-05-29 21:12:51.403010\n",
      "streaming response_cost 3.7e-05\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X litellm.completion 사용하면 proxy server 사용 안하고 하는 거임\n",
    "try:\n",
    "  response = completion(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{ \"content\": \"안녕\",\"role\": \"user\"}],\n",
    "    # stream=True\n",
    "  )\n",
    "except OpenAIError as e:\n",
    "  print('error')\n",
    "  print(e)\n",
    "  print('/error')\n",
    "\n",
    "# print(\"\\n\")\n",
    "print(\"response: \")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"chunks: \")\n",
    "for chunk in response:\n",
    "  print(chunk)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"raw response: \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: \n",
      "In the stillness of the night,\n",
      "Stars twinkle with all their might.\n",
      "Moonlight dances on the sea,\n",
      "A peaceful moment just for me.\n",
      "\n",
      "The world is quiet, calm, and still,\n",
      "A moment to just be, to chill.\n",
      "I breathe in deep, let go of stress,\n",
      "And feel the peace, the happiness.\n",
      "\n",
      "In this moment, I am free,\n",
      "To just be me, to just be.\n",
      "\n",
      "\n",
      "chunks: \n",
      "('id', 'chatcmpl-9UCmUDs5ntmMMpnzkhD8Ycq0rfRZU')\n",
      "element: \n",
      "id\n",
      "element: \n",
      "chatcmpl-9UCmUDs5ntmMMpnzkhD8Ycq0rfRZU\n",
      "('choices', [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='In the stillness of the night,\\nStars twinkle with all their might.\\nMoonlight dances on the sea,\\nA peaceful moment just for me.\\n\\nThe world is quiet, calm, and still,\\nA moment to just be, to chill.\\nI breathe in deep, let go of stress,\\nAnd feel the peace, the happiness.\\n\\nIn this moment, I am free,\\nTo just be me, to just be.', role='assistant', function_call=None, tool_calls=None))])\n",
      "element: \n",
      "choices\n",
      "element: \n",
      "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='In the stillness of the night,\\nStars twinkle with all their might.\\nMoonlight dances on the sea,\\nA peaceful moment just for me.\\n\\nThe world is quiet, calm, and still,\\nA moment to just be, to chill.\\nI breathe in deep, let go of stress,\\nAnd feel the peace, the happiness.\\n\\nIn this moment, I am free,\\nTo just be me, to just be.', role='assistant', function_call=None, tool_calls=None))]\n",
      "('created', 1716985998)\n",
      "element: \n",
      "created\n",
      "element: \n",
      "1716985998\n",
      "('model', 'gpt-3.5-turbo-0125')\n",
      "element: \n",
      "model\n",
      "element: \n",
      "gpt-3.5-turbo-0125\n",
      "('object', 'chat.completion')\n",
      "element: \n",
      "object\n",
      "element: \n",
      "chat.completion\n",
      "('system_fingerprint', None)\n",
      "element: \n",
      "system_fingerprint\n",
      "element: \n",
      "None\n",
      "('usage', CompletionUsage(completion_tokens=84, prompt_tokens=17, total_tokens=101))\n",
      "element: \n",
      "usage\n",
      "element: \n",
      "CompletionUsage(completion_tokens=84, prompt_tokens=17, total_tokens=101)\n",
      "\n",
      "\n",
      "raw response: \n",
      "ChatCompletion(id='chatcmpl-9UCmUDs5ntmMMpnzkhD8Ycq0rfRZU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='In the stillness of the night,\\nStars twinkle with all their might.\\nMoonlight dances on the sea,\\nA peaceful moment just for me.\\n\\nThe world is quiet, calm, and still,\\nA moment to just be, to chill.\\nI breathe in deep, let go of stress,\\nAnd feel the peace, the happiness.\\n\\nIn this moment, I am free,\\nTo just be me, to just be.', role='assistant', function_call=None, tool_calls=None))], created=1716985998, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=84, prompt_tokens=17, total_tokens=101))\n"
     ]
    }
   ],
   "source": [
    "# X `config.yaml`을 사용해서 litellm 사용하려면 `openai.OpenAI` 사용해야함\n",
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"sk-5798\",\n",
    "    base_url=\"http://localhost:4000\"\n",
    ")\n",
    "\n",
    "# request sent to model set on litellm proxy, `litellm --model`\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-temp-0.1\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"this is a test request, write a short poem\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"response: \")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"chunks: \")\n",
    "for chunk in response:\n",
    "  print(chunk)\n",
    "  for element in chunk:\n",
    "    print(\"element: \")\n",
    "    print(element)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"raw response: \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'mock_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\n\u001b[0;32m      6\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-5798\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:4000\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# request sent to model set on litellm proxy, `litellm --model`\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-temp-0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrim_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthis is a test request, write a short poem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-temp-0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmock_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms simple to use and easy to get started\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Completions.create() got an unexpected keyword argument 'mock_response'"
     ]
    }
   ],
   "source": [
    "# X `config.yaml`을 사용해서 litellm 사용하려면 `openai.OpenAI` 사용해야함\n",
    "import openai\n",
    "from litellm.utils import trim_messages\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"sk-5798\",\n",
    "    base_url=\"http://localhost:4000\"\n",
    ")\n",
    "\n",
    "# request sent to model set on litellm proxy, `litellm --model`\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-temp-0.1\", messages = trim_messages([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"this is a test request, write a short poem\"\n",
    "    }\n",
    "], model=\"gpt-3.5-temp-0.1\"))\n",
    "\n",
    "print(\"response: \")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"chunks: \")\n",
    "for chunk in response:\n",
    "  print(chunk)\n",
    "  for element in chunk:\n",
    "    print(\"element: \")\n",
    "    print(element)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"raw response: \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9UCmXVCgU3CKIgSAydZP9XbmneSCi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='안녕하세요! 무엇을 도와드릴까요?', role='assistant', function_call=None, tool_calls=None))], created=1716986001, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n",
      "ChatCompletion(id='chatcmpl-9UCmeDaiitRPEDDN9NFHZZKVbuzNY', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='안녕하세요! 무엇을 도와드릴까요?', role='assistant', function_call=None, tool_calls=None))], created=1716986008, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=11, total_tokens=32))\n",
      "안녕하세요! 무엇을 도와드릴까요?\n"
     ]
    }
   ],
   "source": [
    "from litellm.utils import trim_messages\n",
    "\n",
    "def completion (*args, **kwargs):\n",
    "\t return client.chat.completions.create(*args, **kwargs)\n",
    "\n",
    "print(completion(model=\"gpt-3.5-temp-0.1\", messages=[{ \"content\": \"안녕\", \"role\": \"user\"}]))\n",
    "\n",
    "def getResponse (message, *args, **kwargs):\n",
    "\t return completion(model=\"gpt-3.5-temp-0.1\", messages=[{ \"content\": message, \"role\": \"user\"}], *args, **kwargs)\n",
    "\n",
    "print(getResponse(\"안녕\"))\n",
    "\n",
    "def getOnlyResponse (message, *args, **kwargs):\n",
    "\t return getResponse(message, *args, **kwargs).choices[0].message.content\n",
    "\n",
    "print(getOnlyResponse(\"안녕\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안\n",
      "녕\n",
      " 친\n",
      "구\n",
      "야\n",
      "!\n",
      " 어\n",
      "떻\n",
      "게\n",
      " 지\n",
      "내\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def printOnlyResponseStreaming(message, *args, **kwargs):\n",
    "\tresponse = getResponse(message, stream=True, *args, **kwargs)\n",
    "\tfor part in response:\n",
    "\t\tprint(part.choices[0].delta.content or \"\")\n",
    "\n",
    "printOnlyResponseStreaming(\"안녕 스폰지밥? (대답은 스폰지밥처럼 하시오)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안\n",
      "녕\n",
      " 친\n",
      "구\n",
      "야\n",
      "!\n",
      " 어\n",
      "떻\n",
      "게\n",
      " 지\n",
      "내\n",
      "?\n",
      "\n",
      "\n",
      "Full response:  안녕 친구야! 어떻게 지내?\n"
     ]
    }
   ],
   "source": [
    "# 스트리밍 응답을 빌드하는 함수 정의\n",
    "def build_streamed_response(chunks):\n",
    "    # 모든 청크를 합쳐서 하나의 응답으로 빌드\n",
    "    return \"\".join(chunk.choices[0].delta.content for chunk in chunks if chunk.choices[0].delta.content is not None)\n",
    "\n",
    "# 스트리밍 응답을 출력하는 함수\n",
    "def print_streaming_response(message, *args, **kwargs):\n",
    "    try:\n",
    "        response = getResponse(message, stream=True, *args, **kwargs)\n",
    "        chunks = []\n",
    "        for chunk in response:\n",
    "            print(chunk.choices[0].delta.content or \"\")\n",
    "            chunks.append(chunk)\n",
    "        full_response = build_streamed_response(chunks)\n",
    "        print(\"\\nFull response: \", full_response)\n",
    "    except OpenAIError as e:\n",
    "        print(\"Error: \", e)\n",
    "\n",
    "# 함수 호출\n",
    "print_streaming_response(\"안녕 스폰지밥? (대답은 스폰지밥처럼 하시오)\")\n",
    "\n",
    "# def getOnlyResponseStreaming(message, *args, **kwargs):\n",
    "# \tresponse = getResponse(message, stream=True, *args, **kwargs)\n",
    "\n",
    "# \tchunks = []\n",
    "# \tfor chunk in response:\n",
    "# \t\tchunks.append(chunk)\n",
    "\n",
    "# \treturn litellm.stream_chunk_builder(chunks, messages=[{ \"content\": message, \"role\": \"user\"}])\n",
    "\n",
    "# print(getOnlyResponseStreaming(\"안녕 스폰지밥? (대답은 스폰지밥처럼 하시오)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X 일단 버려\n",
    "# Spend Tracking\n",
    "import requests\n",
    "url = 'http://localhost:4000/global/spend/report'\n",
    "params = {\n",
    "    'start_date': '2023-04-01',\n",
    "    'end_date': '2024-06-30',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer sk-5798'\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "# response = requests.get(url, headers=headers, params=params)\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "spend_report = response.json()\n",
    "\n",
    "print(spend_report)\n",
    "\n",
    "for row in spend_report:\n",
    "  print(f'row: {row}')\n",
    "\n",
    "  date = row[\"group_by_day\"]\n",
    "  teams = row[\"teams\"]\n",
    "  for team in teams:\n",
    "      team_name = team[\"team_name\"]\n",
    "      total_spend = team[\"total_spend\"]\n",
    "      metadata = team[\"metadata\"]\n",
    "\n",
    "      print(f\"Date: {date}\")\n",
    "      print(f\"Team: {team_name}\")\n",
    "      print(f\"Total Spend: {total_spend}\")\n",
    "      print(\"Metadata: \", metadata)\n",
    "      print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
